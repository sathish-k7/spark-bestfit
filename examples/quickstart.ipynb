{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# spark-bestfit Quick Start\n\nThis notebook demonstrates how to use spark-bestfit to fit statistical distributions to your data using Apache Spark."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom pyspark.sql import SparkSession\n\nfrom spark_bestfit import DistributionFitter\n\n# Create Spark session\nspark = SparkSession.builder.appName(\"DistFitDemo\").getOrCreate()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create data from a known distribution (normal) to verify the fitting works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             value|\n",
      "+------------------+\n",
      "| 54.96714153011233|\n",
      "| 48.61735698828815|\n",
      "| 56.47688538100692|\n",
      "| 65.23029856408026|\n",
      "|47.658466252766644|\n",
      "+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedWriter name=5>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dustin/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 200, in manager\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=50, scale=10, size=100_000)\n",
    "\n",
    "df = spark.createDataFrame([(float(x),) for x in data], [\"value\"])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Fit distributions with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fitter = DistributionFitter()\nresults = fitter.fit(df, column=\"value\", max_distributions=25)\n\n# Get best fit\nbest = results.best(n=1)[0]\nprint(f\"Best distribution: {best.distribution}\")\nprint(f\"SSE: {best.sse:.6f}\")\nprint(f\"Parameters: {best.parameters}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. fatiguelife          SSE=0.000011\n",
      "2. f                    SSE=0.000012\n",
      "3. erlang               SSE=0.000014\n",
      "4. crystalball          SSE=0.000015\n",
      "5. exponnorm            SSE=0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==================================================>       (7 + 1) / 8]"
     ]
    }
   ],
   "source": [
    "# Top 5 distributions\n",
    "for i, result in enumerate(results.best(n=5), 1):\n",
    "    print(f\"{i}. {result.distribution:20s} SSE={result.sse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Custom Fitting Parameters\n\nYou can customize fitting behavior by passing parameters directly to the `fit()` method."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom fitting with specific parameters\nfitter_custom = DistributionFitter(spark, random_seed=123)\nresults_custom = fitter_custom.fit(\n    df,\n    column=\"value\",\n    bins=100,\n    use_rice_rule=False,\n    enable_sampling=True,\n    sample_fraction=0.5,\n    max_sample_size=500_000,\n    max_distributions=35,\n)\n\nbest_custom = results_custom.best(n=1)[0]\nprint(f\"Best: {best_custom.distribution} (SSE={best_custom.sse:.6f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Plotting\n\nVisualize the fitted distribution against the data histogram."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot the best fitting distribution\nfitter.plot(\n    best,\n    df,\n    \"value\",\n    figsize=(14, 8),\n    dpi=100,\n    show_histogram=True,\n    histogram_alpha=0.6,\n    pdf_linewidth=3,\n    title_fontsize=16,\n    label_fontsize=12,\n    grid_alpha=0.3,\n    title=\"Best Fit Distribution\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Data Example\n",
    "\n",
    "When your data is strictly non-negative (e.g., prices, durations), use `support_at_zero = true` to only fit appropriate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate exponential data (non-negative)\ndata_exp = np.random.exponential(scale=5, size=100_000)\ndf_exp = spark.createDataFrame([(float(x),) for x in data_exp], [\"value\"])\n\n# Fit only non-negative distributions using support_at_zero=True\nfitter_exp = DistributionFitter(spark)\nresults_exp = fitter_exp.fit(\n    df_exp,\n    column=\"value\",\n    bins=75,\n    support_at_zero=True,  # Only non-negative distributions\n    enable_sampling=True,\n    max_distributions=10,\n)\n\nprint(\"Top 5 non-negative distributions:\")\nfor i, result in enumerate(results_exp.best(n=5), 1):\n    print(f\"{i}. {result.distribution:20s} SSE={result.sse:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fitted Distributions\n",
    "\n",
    "Once you have a fitted distribution, you can use it to generate samples or evaluate PDF/CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data - mean: 50.01, std: 10.01\n",
      "Fitted samples - mean: 50.15, std: 10.16\n"
     ]
    }
   ],
   "source": [
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "\n",
    "print(f\"Original data - mean: {data.mean():.2f}, std: {data.std():.2f}\")\n",
    "print(f\"Fitted samples - mean: {samples.mean():.2f}, std: {samples.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF and CDF values:\n",
      "  x=30: PDF=0.005296, CDF=0.0214\n",
      "  x=40: PDF=0.024297, CDF=0.1575\n",
      "  x=50: PDF=0.039399, CDF=0.4974\n",
      "  x=60: PDF=0.024089, CDF=0.8341\n",
      "  x=70: PDF=0.005893, CDF=0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==================================================>       (7 + 1) / 8]"
     ]
    }
   ],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"PDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all results to pandas DataFrame for further analysis\n",
    "results_df = results.to_pandas()\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
