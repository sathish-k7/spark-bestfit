{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# spark-bestfit API Demo\n\nThis notebook demonstrates the complete API for the `spark-bestfit` library, including:\n\n1. **Distribution Fitting** - Using DistributionFitter with direct parameters\n2. **Working with Results** - FitResults and DistributionFitResult objects\n3. **Plotting** - Visualization with customizable parameters\n4. **Excluding Distributions** - Customizing which distributions to fit"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's create a Spark session. Note: **You** are responsible for creating and configuring your SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session (your responsibility - configure as needed for your environment)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"API-Demo\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import spark-bestfit components\nfrom spark_bestfit import (\n    DistributionFitter,\n    DEFAULT_EXCLUDED_DISTRIBUTIONS,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample data from known distributions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution data\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=50_000)\n",
    "df_normal = spark.createDataFrame([(float(x),) for x in normal_data], [\"value\"])\n",
    "\n",
    "# Exponential distribution data (non-negative)\n",
    "exp_data = np.random.exponential(scale=5, size=50_000)\n",
    "df_exp = spark.createDataFrame([(float(x),) for x in exp_data], [\"value\"])\n",
    "\n",
    "# Gamma distribution data\n",
    "gamma_data = np.random.gamma(shape=2.0, scale=2.0, size=50_000)\n",
    "df_gamma = spark.createDataFrame([(float(x),) for x in gamma_data], [\"value\"])\n",
    "\n",
    "print(f\"Normal data: {df_normal.count():,} rows, mean={normal_data.mean():.2f}, std={normal_data.std():.2f}\")\n",
    "print(f\"Exponential data: {df_exp.count():,} rows, mean={exp_data.mean():.2f}\")\n",
    "print(f\"Gamma data: {df_gamma.count():,} rows, mean={gamma_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 1: Excluded Distributions\n\nspark-bestfit excludes some slow distributions by default. You can customize this."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 DEFAULT_EXCLUDED_DISTRIBUTIONS\n\nSome distributions are excluded by default because they are very slow to fit."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# View default excluded distributions\nprint(f\"Default excluded distributions ({len(DEFAULT_EXCLUDED_DISTRIBUTIONS)}):\")\nfor dist in sorted(DEFAULT_EXCLUDED_DISTRIBUTIONS):\n    print(f\"  - {dist}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Include a specific distribution that's excluded by default\ncustom_exclusions = tuple(d for d in DEFAULT_EXCLUDED_DISTRIBUTIONS if d != \"wald\")\n\nfitter_with_wald = DistributionFitter(spark, excluded_distributions=custom_exclusions)\nprint(f\"Now fitting 'wald' distribution (removed from exclusions)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Distribution Fitting\n",
    "\n",
    "The `DistributionFitter` class is the main entry point for fitting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fitter with default config\n",
    "fitter = DistributionFitter(spark)\n",
    "\n",
    "# Fit distributions to normal data (limit to 20 for demo speed)\n",
    "print(\"Fitting distributions to normal data...\")\n",
    "results_normal = fitter.fit(df_normal, column=\"value\", max_distributions=20)\n",
    "\n",
    "print(f\"\\nFitted {results_normal.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Fitting with Custom Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fit only non-negative distributions using support_at_zero=True\nfitter_nonneg = DistributionFitter(spark)\n\nprint(\"Fitting non-negative distributions to exponential data...\")\nresults_exp = fitter_nonneg.fit(\n    df_exp,\n    column=\"value\",\n    bins=100,\n    support_at_zero=True,  # Only fit non-negative distributions\n    enable_sampling=True,\n    max_distributions=15,\n)\n\nprint(f\"Fitted {results_exp.count()} non-negative distributions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using Active SparkSession\n",
    "\n",
    "If a SparkSession is already active, you don't need to pass it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistributionFitter can use the active session automatically\n",
    "fitter_active = DistributionFitter()  # No spark parameter needed\n",
    "print(f\"Using active session: {fitter_active.spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Working with Results\n",
    "\n",
    "The `fit()` method returns a `FitResults` object for easy result manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Getting Best Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best distribution by SSE (default)\n",
    "best_sse = results_normal.best(n=1)[0]\n",
    "print(f\"Best by SSE: {best_sse.distribution}\")\n",
    "print(f\"  SSE: {best_sse.sse:.6f}\")\n",
    "print(f\"  AIC: {best_sse.aic:.2f}\")\n",
    "print(f\"  BIC: {best_sse.bic:.2f}\")\n",
    "print(f\"  Parameters: {[f'{p:.4f}' for p in best_sse.parameters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 by different metrics\n",
    "print(\"Top 5 by SSE:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"sse\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} SSE={r.sse:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 by AIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"aic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} AIC={r.aic:.2f}\")\n",
    "\n",
    "print(\"\\nTop 5 by BIC:\")\n",
    "for i, r in enumerate(results_normal.best(n=5, metric=\"bic\"), 1):\n",
    "    print(f\"  {i}. {r.distribution:20s} BIC={r.bic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by SSE threshold\n",
    "good_fits = results_normal.filter(sse_threshold=0.01)\n",
    "print(f\"Distributions with SSE < 0.01: {good_fits.count()}\")\n",
    "\n",
    "for r in good_fits.best(n=10):\n",
    "    print(f\"  {r.distribution:20s} SSE={r.sse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for further analysis\n",
    "df_results = results_normal.to_pandas()\n",
    "print(\"Results as pandas DataFrame:\")\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Using Fitted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DistributionFitResult object wraps the scipy.stats distribution\n",
    "best = results_normal.best(n=1)[0]\n",
    "\n",
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "print(f\"Generated {len(samples)} samples from fitted {best.distribution}\")\n",
    "print(f\"  Sample mean: {samples.mean():.2f} (original: {normal_data.mean():.2f})\")\n",
    "print(f\"  Sample std: {samples.std():.2f} (original: {normal_data.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"PDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Plotting\n",
    "\n",
    "Visualize the fitted distribution with the data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot with default config\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    title=\"Best Fit Distribution (Normal Data)\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.2 Plot with Custom Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom plot with direct parameters\nfig, ax = fitter.plot(\n    best,\n    df_normal,\n    \"value\",\n    figsize=(14, 8),\n    dpi=100,\n    histogram_alpha=0.7,\n    pdf_linewidth=3,\n    title_fontsize=18,\n    label_fontsize=14,\n    legend_fontsize=12,\n    grid_alpha=0.4,\n    title=\"Distribution Fit with Custom Styling\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Plot Non-Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best fit for exponential data\nbest_exp = results_exp.best(n=1)[0]\nprint(f\"Best fit for exponential data: {best_exp.distribution}\")\n\nfig, ax = fitter_nonneg.plot(\n    best_exp,\n    df_exp,\n    \"value\",\n    figsize=(14, 8),\n    dpi=100,\n    histogram_alpha=0.7,\n    pdf_linewidth=3,\n    title_fontsize=18,\n    title=f\"Best Fit: {best_exp.distribution.capitalize()}\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Complete Workflow Example\n",
    "\n",
    "Putting it all together - a complete production-style workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete workflow with all parameters\nfitter_gamma = DistributionFitter(spark, random_seed=42)\n\n# Fit distributions\nprint(\"Fitting gamma distribution data...\")\nresults = fitter_gamma.fit(\n    df_gamma,\n    column=\"value\",\n    bins=100,\n    use_rice_rule=False,\n    enable_sampling=True,\n    max_sample_size=1_000_000,\n    max_distributions=25,\n)\n\n# Get best result\nbest = results.best(n=1)[0]\nprint(f\"\\nBest distribution: {best.distribution}\")\nprint(f\"SSE: {best.sse:.6f}\")\nprint(f\"Parameters: {[f'{p:.4f}' for p in best.parameters]}\")\n\n# Plot with custom parameters\nfig, ax = fitter_gamma.plot(\n    best,\n    df_gamma,\n    \"value\",\n    figsize=(14, 9),\n    dpi=150,\n    histogram_alpha=0.6,\n    pdf_linewidth=3,\n    title_fontsize=16,\n    title=f\"Gamma Data - Best Fit: {best.distribution.capitalize()}\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()\n\n# Show top 5 results\nprint(\"\\nTop 5 distributions:\")\ndf_top5 = results.to_pandas().head(5)\ndf_top5[[\"distribution\", \"sse\", \"aic\", \"bic\"]]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\nThis notebook demonstrated:\n\n1. **Excluded Distributions**:\n   - `DEFAULT_EXCLUDED_DISTRIBUTIONS` - Slow distributions excluded by default\n   - Pass custom `excluded_distributions` to `DistributionFitter()` to include/exclude\n\n2. **SparkSession Management**:\n   - You create and configure your own SparkSession\n   - Pass it to `DistributionFitter(spark)` or use active session\n\n3. **Fitting**:\n   - `DistributionFitter.fit()` - Fit distributions to data\n   - Parameters: `bins`, `use_rice_rule`, `support_at_zero`, `enable_sampling`, etc.\n   - `max_distributions` parameter to limit fitting scope\n\n4. **Results**:\n   - `results.best(n, metric)` - Get top N by SSE/AIC/BIC\n   - `results.filter()` - Filter by threshold\n   - `results.to_pandas()` - Convert to pandas DataFrame\n   - `DistributionFitResult.sample()`, `.pdf()`, `.cdf()` - Use fitted distribution\n\n5. **Plotting**:\n   - `fitter.plot()` - Visualize fitted distribution with data histogram\n   - Customizable with `figsize`, `dpi`, `histogram_alpha`, `pdf_linewidth`, etc."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
