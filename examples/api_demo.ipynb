{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# spark-bestfit API Demo\n\nThis notebook demonstrates the complete API for the `spark-bestfit` library, including:\n\n1. **Distribution Fitting** - Using DistributionFitter with direct parameters\n2. **Working with Results** - FitResults and DistributionFitResult objects\n3. **Plotting** - Visualization with customizable parameters\n4. **Excluding Distributions** - Customizing which distributions to fit"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's create a Spark session. Note: **You** are responsible for creating and configuring your SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session (your responsibility - configure as needed for your environment)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"API-Demo\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import spark-bestfit components\nfrom spark_bestfit import (\n    DistributionFitter,\n    DEFAULT_EXCLUDED_DISTRIBUTIONS,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample data from known distributions for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Normal distribution data\n",
    "normal_data = np.random.normal(loc=50, scale=10, size=50_000)\n",
    "df_normal = spark.createDataFrame([(float(x),) for x in normal_data], [\"value\"])\n",
    "\n",
    "# Exponential distribution data (non-negative)\n",
    "exp_data = np.random.exponential(scale=5, size=50_000)\n",
    "df_exp = spark.createDataFrame([(float(x),) for x in exp_data], [\"value\"])\n",
    "\n",
    "# Gamma distribution data\n",
    "gamma_data = np.random.gamma(shape=2.0, scale=2.0, size=50_000)\n",
    "df_gamma = spark.createDataFrame([(float(x),) for x in gamma_data], [\"value\"])\n",
    "\n",
    "print(f\"Normal data: {df_normal.count():,} rows, mean={normal_data.mean():.2f}, std={normal_data.std():.2f}\")\n",
    "print(f\"Exponential data: {df_exp.count():,} rows, mean={exp_data.mean():.2f}\")\n",
    "print(f\"Gamma data: {df_gamma.count():,} rows, mean={gamma_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 1: Excluded Distributions\n\nspark-bestfit excludes some slow distributions by default. You can customize this."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 DEFAULT_EXCLUDED_DISTRIBUTIONS\n\nSome distributions are excluded by default because they are very slow to fit."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# View default excluded distributions\nprint(f\"Default excluded distributions ({len(DEFAULT_EXCLUDED_DISTRIBUTIONS)}):\")\nfor dist in sorted(DEFAULT_EXCLUDED_DISTRIBUTIONS):\n    print(f\"  - {dist}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Include a specific distribution that's excluded by default\ncustom_exclusions = tuple(d for d in DEFAULT_EXCLUDED_DISTRIBUTIONS if d != \"wald\")\n\nfitter_with_wald = DistributionFitter(spark, excluded_distributions=custom_exclusions)\nprint(f\"Now fitting 'wald' distribution (removed from exclusions)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Distribution Fitting\n",
    "\n",
    "The `DistributionFitter` class is the main entry point for fitting distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fitter with default config\n",
    "fitter = DistributionFitter(spark)\n",
    "\n",
    "# Fit distributions to normal data (limit to 20 for demo speed)\n",
    "print(\"Fitting distributions to normal data...\")\n",
    "results_normal = fitter.fit(df_normal, column=\"value\", max_distributions=20)\n",
    "\n",
    "print(f\"\\nFitted {results_normal.count()} distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Fitting with Custom Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fit only non-negative distributions using support_at_zero=True\nfitter_nonneg = DistributionFitter(spark)\n\nprint(\"Fitting non-negative distributions to exponential data...\")\nresults_exp = fitter_nonneg.fit(\n    df_exp,\n    column=\"value\",\n    bins=100,\n    support_at_zero=True,  # Only fit non-negative distributions\n    enable_sampling=True,\n    max_distributions=15,\n)\n\nprint(f\"Fitted {results_exp.count()} non-negative distributions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using Active SparkSession\n",
    "\n",
    "If a SparkSession is already active, you don't need to pass it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistributionFitter can use the active session automatically\n",
    "fitter_active = DistributionFitter()  # No spark parameter needed\n",
    "print(f\"Using active session: {fitter_active.spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Working with Results\n",
    "\n",
    "The `fit()` method returns a `FitResults` object for easy result manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Getting Best Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get best distribution (by K-S statistic, the default)\nbest = results_normal.best(n=1)[0]\nprint(f\"Best by K-S statistic: {best.distribution}\")\nprint(f\"  K-S statistic: {best.ks_statistic:.6f}\")\nprint(f\"  p-value: {best.pvalue:.4f}\")\nprint(f\"  SSE: {best.sse:.6f}\")\nprint(f\"  AIC: {best.aic:.2f}\")\nprint(f\"  BIC: {best.bic:.2f}\")\nprint(f\"  Parameters: {[f'{p:.4f}' for p in best.parameters]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get top 5 by different metrics\nprint(\"Top 5 by K-S statistic (default):\")\nfor i, r in enumerate(results_normal.best(n=5), 1):\n    print(f\"  {i}. {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n\nprint(\"\\nTop 5 by SSE:\")\nfor i, r in enumerate(results_normal.best(n=5, metric=\"sse\"), 1):\n    print(f\"  {i}. {r.distribution:20s} SSE={r.sse:.6f}\")\n\nprint(\"\\nTop 5 by AIC:\")\nfor i, r in enumerate(results_normal.best(n=5, metric=\"aic\"), 1):\n    print(f\"  {i}. {r.distribution:20s} AIC={r.aic:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Filtering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter by K-S statistic threshold\ngood_fits = results_normal.filter(ks_threshold=0.05)\nprint(f\"Distributions with K-S statistic < 0.05: {good_fits.count()}\")\n\nfor r in good_fits.best(n=10):\n    print(f\"  {r.distribution:20s} KS={r.ks_statistic:.6f} p={r.pvalue:.4f}\")\n\n# Filter by p-value threshold (keep distributions with p-value > 0.05)\nsignificant = results_normal.filter(pvalue_threshold=0.05)\nprint(f\"\\nDistributions with p-value > 0.05: {significant.count()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Converting to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to pandas DataFrame for further analysis\ndf_results = results_normal.df.toPandas()\nprint(\"Results as pandas DataFrame:\")\ndf_results.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Using Fitted Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DistributionFitResult object wraps the scipy.stats distribution\n",
    "best = results_normal.best(n=1)[0]\n",
    "\n",
    "# Generate samples from the fitted distribution\n",
    "samples = best.sample(size=10000, random_state=42)\n",
    "print(f\"Generated {len(samples)} samples from fitted {best.distribution}\")\n",
    "print(f\"  Sample mean: {samples.mean():.2f} (original: {normal_data.mean():.2f})\")\n",
    "print(f\"  Sample std: {samples.std():.2f} (original: {normal_data.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PDF at specific points\n",
    "x = np.array([30, 40, 50, 60, 70])\n",
    "pdf_values = best.pdf(x)\n",
    "cdf_values = best.cdf(x)\n",
    "\n",
    "print(\"PDF and CDF values:\")\n",
    "for xi, pdf, cdf in zip(x, pdf_values, cdf_values):\n",
    "    print(f\"  x={xi}: PDF={pdf:.6f}, CDF={cdf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Plotting\n",
    "\n",
    "Visualize the fitted distribution with the data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Basic Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot with default config\n",
    "fig, ax = fitter.plot(\n",
    "    best,\n",
    "    df_normal,\n",
    "    \"value\",\n",
    "    title=\"Best Fit Distribution (Normal Data)\",\n",
    "    xlabel=\"Value\",\n",
    "    ylabel=\"Density\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4.2 Plot with Custom Parameters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom plot with direct parameters\nfig, ax = fitter.plot(\n    best,\n    df_normal,\n    \"value\",\n    figsize=(14, 8),\n    dpi=100,\n    histogram_alpha=0.7,\n    pdf_linewidth=3,\n    title_fontsize=18,\n    label_fontsize=14,\n    legend_fontsize=12,\n    grid_alpha=0.4,\n    title=\"Distribution Fit with Custom Styling\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Plot Non-Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best fit for exponential data\nbest_exp = results_exp.best(n=1)[0]\nprint(f\"Best fit for exponential data: {best_exp.distribution}\")\nprint(f\"  K-S statistic: {best_exp.ks_statistic:.6f}\")\nprint(f\"  p-value: {best_exp.pvalue:.4f}\")\n\nfig, ax = fitter_nonneg.plot(\n    best_exp,\n    df_exp,\n    \"value\",\n    figsize=(14, 8),\n    dpi=100,\n    histogram_alpha=0.7,\n    pdf_linewidth=3,\n    title_fontsize=18,\n    title=f\"Best Fit: {best_exp.distribution.capitalize()}\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "## 4.4 Q-Q Plots for Goodness-of-Fit Assessment\n\nA Q-Q (quantile-quantile) plot is a powerful visual tool for assessing how well a distribution fits your data. It plots sample quantiles against theoretical quantiles from the fitted distribution. If the fit is good, points will fall approximately along the diagonal reference line.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Q-Q plot for the best fit on normal data\nfig, ax = fitter.plot_qq(\n    best,\n    df_normal,\n    \"value\",\n    max_points=1000,  # Sample size for plotting (too many points clutters the plot)\n    figsize=(10, 10),\n    title=\"Q-Q Plot: Normal Data vs Fitted Distribution\",\n)\nplt.show()\n\n# Compare: Q-Q plot for exponential data\nfig, ax = fitter_nonneg.plot_qq(\n    best_exp,\n    df_exp,\n    \"value\",\n    max_points=1000,\n    figsize=(10, 10),\n    title=\"Q-Q Plot: Exponential Data vs Fitted Distribution\",\n)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Complete Workflow Example\n",
    "\n",
    "Putting it all together - a complete production-style workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete workflow with all parameters\nfitter_gamma = DistributionFitter(spark, random_seed=42)\n\n# Fit distributions\nprint(\"Fitting gamma distribution data...\")\nresults = fitter_gamma.fit(\n    df_gamma,\n    column=\"value\",\n    bins=100,\n    use_rice_rule=False,\n    enable_sampling=True,\n    max_sample_size=1_000_000,\n    max_distributions=25,\n)\n\n# Get best result\nbest = results.best(n=1)[0]\nprint(f\"\\nBest distribution: {best.distribution}\")\nprint(f\"K-S statistic: {best.ks_statistic:.6f}\")\nprint(f\"p-value: {best.pvalue:.4f}\")\nprint(f\"SSE: {best.sse:.6f}\")\nprint(f\"Parameters: {[f'{p:.4f}' for p in best.parameters]}\")\n\n# Plot with custom parameters\nfig, ax = fitter_gamma.plot(\n    best,\n    df_gamma,\n    \"value\",\n    figsize=(14, 9),\n    dpi=150,\n    histogram_alpha=0.6,\n    pdf_linewidth=3,\n    title_fontsize=16,\n    title=f\"Gamma Data - Best Fit: {best.distribution.capitalize()}\",\n    xlabel=\"Value\",\n    ylabel=\"Density\",\n)\nplt.show()\n\n# Show top 5 results (sorted by K-S statistic for meaningful ranking)\nprint(\"\\nTop 5 distributions:\")\ndf_top5 = results.df.toPandas().sort_values(\"ks_statistic\").head(5)\ndf_top5[[\"distribution\", \"ks_statistic\", \"pvalue\", \"sse\", \"aic\", \"bic\"]]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\nThis notebook demonstrated:\n\n1. **Excluded Distributions**:\n   - `DEFAULT_EXCLUDED_DISTRIBUTIONS` - Slow distributions excluded by default\n   - Pass custom `excluded_distributions` to `DistributionFitter()` to include/exclude\n\n2. **SparkSession Management**:\n   - You create and configure your own SparkSession\n   - Pass it to `DistributionFitter(spark)` or use active session\n\n3. **Fitting**:\n   - `DistributionFitter.fit()` - Fit distributions to data\n   - Parameters: `bins`, `use_rice_rule`, `support_at_zero`, `enable_sampling`, etc.\n   - `max_distributions` parameter to limit fitting scope\n\n4. **Results**:\n   - `results.best(n, metric)` - Get top N by K-S statistic (default), SSE, AIC, or BIC\n   - `results.filter(ks_threshold, pvalue_threshold)` - Filter by goodness-of-fit\n   - `results.df.toPandas()` - Convert to pandas DataFrame\n   - `DistributionFitResult.sample()`, `.pdf()`, `.cdf()` - Use fitted distribution\n\n5. **Plotting**:\n   - `fitter.plot()` - Visualize fitted distribution with data histogram\n   - `fitter.plot_qq()` - Q-Q plot for visual goodness-of-fit assessment\n   - Customizable with `figsize`, `dpi`, `histogram_alpha`, `pdf_linewidth`, etc.\n\n6. **Goodness-of-Fit Metrics**:\n   - **K-S statistic** (default) - Lower is better, measures max distance from empirical CDF\n   - **p-value** - Higher is better (>0.05 suggests good fit)\n   - **SSE** - Sum of squared errors between histogram and fitted PDF\n   - **AIC/BIC** - Information criteria for model comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary\n\nThis notebook demonstrated:\n\n1. **Excluded Distributions**:\n   - `DEFAULT_EXCLUDED_DISTRIBUTIONS` - Slow distributions excluded by default\n   - Pass custom `excluded_distributions` to `DistributionFitter()` to include/exclude\n\n2. **SparkSession Management**:\n   - You create and configure your own SparkSession\n   - Pass it to `DistributionFitter(spark)` or use active session\n\n3. **Fitting**:\n   - `DistributionFitter.fit()` - Fit distributions to data\n   - Parameters: `bins`, `use_rice_rule`, `support_at_zero`, `enable_sampling`, etc.\n   - `max_distributions` parameter to limit fitting scope\n\n4. **Results**:\n   - `results.best(n, metric)` - Get top N by K-S statistic (default), SSE, AIC, or BIC\n   - `results.filter(ks_threshold, pvalue_threshold)` - Filter by goodness-of-fit\n   - `results.df.toPandas()` - Convert to pandas DataFrame\n   - `DistributionFitResult.sample()`, `.pdf()`, `.cdf()` - Use fitted distribution\n\n5. **Plotting**:\n   - `fitter.plot()` - Visualize fitted distribution with data histogram\n   - Customizable with `figsize`, `dpi`, `histogram_alpha`, `pdf_linewidth`, etc.\n\n6. **Goodness-of-Fit Metrics**:\n   - **K-S statistic** (default) - Lower is better, measures max distance from empirical CDF\n   - **p-value** - Higher is better (>0.05 suggests good fit)\n   - **SSE** - Sum of squared errors between histogram and fitted PDF\n   - **AIC/BIC** - Information criteria for model comparison"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
